from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time
import subprocess
import pandas as pd
from threading import Thread
from datetime import datetime

# ===========================
# ğŸ”§ GPUãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–¢æ•°ï¼ˆè¡¨å½¢å¼ï¼‰
# ===========================
gpu_log = []
timestamps = []
monitoring_flag = False

def monitor_gpu_utilization(interval=1):
    global gpu_log, timestamps
    while monitoring_flag:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],
            stdout=subprocess.PIPE,
            encoding='utf-8'
        )
        usage = [int(line.strip()) for line in result.stdout.strip().split('\n')]
        gpu_log.append(usage)
        timestamps.append(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        time.sleep(interval)

def save_gpu_log_to_csv():
    if gpu_log:
        df = pd.DataFrame(gpu_log, columns=[f"GPU {i}" for i in range(len(gpu_log[0]))])
        df.insert(0, "Timestamp", timestamps)
        
        filename = f"gpu_log_wide_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        df.to_csv(filename, index=False)
        
        print(f"ğŸ“„ GPUä½¿ç”¨ç‡ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆãƒ¯ã‚¤ãƒ‰å½¢å¼ï¼‰: {filename}")

# ===========================
# ğŸ”§ ãƒ¢ãƒ‡ãƒ«æº–å‚™
# ===========================
model_path = "./RakutenAI-2.0-8x7B-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# GPUæƒ…å ±å‡ºåŠ›
print("ğŸŸ¢ CUDA åˆ©ç”¨å¯èƒ½ :", torch.cuda.is_available())
print(" ä½¿ç”¨å¯èƒ½GPUæ•° :", torch.cuda.device_count())
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f"  - GPU {i}: {torch.cuda.get_device_name(i)}")
print("\nãƒãƒ£ãƒƒãƒˆé–‹å§‹ï¼ï¼ˆq ã§çµ‚äº†ï¼‰")

# ===========================
# ğŸ”§ å¯¾è©±ãƒ«ãƒ¼ãƒ—
# ===========================
while True:
    user_input = input("ãƒ¦ãƒ¼ã‚¶ãƒ¼ > ")
    if user_input.strip().lower() == "q":
        break

    prompt = f"""ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ä¼šè©±ã§ã™ã€‚\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"""
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = inputs["input_ids"]
    generated_ids = input_ids.clone()
    max_new_tokens = 256

    # ğŸ” ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–‹å§‹
    gpu_log = []
    timestamps = []
    monitoring_flag = True
    monitor_thread = Thread(target=monitor_gpu_utilization)
    monitor_thread.start()

    start_time = time.time()
    with torch.no_grad():
        for _ in range(max_new_tokens):
            outputs = model(input_ids=generated_ids)
            next_token_logits = outputs.logits[:, -1, :]
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)
            if next_token_id.item() == tokenizer.eos_token_id:
                break
            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)
    end_time = time.time()

    monitoring_flag = False
    monitor_thread.join()
    save_gpu_log_to_csv()

    # å¿œç­”å‡ºåŠ›
    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True).split("ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:")[-1].strip()
    print("ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ >", response)
    print(f"ğŸ”¹ å¿œç­”ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f} ç§’\n")
