from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = "./RakutenAI-2.0-8x7B-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("ãƒãƒ£ãƒƒãƒˆé–‹å§‹ï¼ï¼ˆq ã§çµ‚äº†ï¼‰")

while True:
    user_input = input("ãƒ¦ãƒ¼ã‚¶ãƒ¼ > ")
    if user_input.strip().lower() == "q":
        break

    # ä¼šè©±å½¢å¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ±ä¸€ï¼ˆ"RakutenAI" è¡¨ç¤ºãªã—ï¼‰
    prompt = f"""ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ä¼šè©±ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input}
ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:"""

    # å…¥åŠ›ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = inputs["input_ids"]
    generated_ids = input_ids.clone()

    max_new_tokens = 256  # ğŸ”ºã“ã“ã‚’å¤§ããã™ã‚‹ã“ã¨ã§é€”ä¸­ã§åˆ‡ã‚Œã«ãããªã‚‹

    with torch.no_grad():
        for _ in range(max_new_tokens):
            outputs = model(input_ids=generated_ids)
            next_token_logits = outputs.logits[:, -1, :]
            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)

            if next_token_id.item() == tokenizer.eos_token_id:
                break

            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)

    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹éƒ¨åˆ†ã®ã¿æŠ½å‡º
    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    response = decoded.split("ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ:")[-1].strip()

    print("ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ >", response)
